{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Import Lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.misc import imresize, imread\n",
    "import itertools\n",
    "import sklearn\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from scipy.misc import imresize\n",
    "from skimage import transform,data\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "%matplotlib inline\n",
    "plt.style.use({'figure.figsize':(50,50)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Import Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv2.imread('../input/Pic/example.png')\n",
    "X1 = cv2.imread('../input/Pic/example.png')\n",
    "#X = X.reshape((1,) + X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.imshow(image) # 显示图片(if you want)\\nplt.axis('on') # 显示坐标轴\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.imshow(image) # 显示图片(if you want)\n",
    "plt.axis('on') # 显示坐标轴\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('../input/X.npy') # images\n",
    "Y = np.load('../input/Y.npy') # labels associated to images (0 = non-tumor, 1 = Tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_reshape = to_categorical(Y,num_classes = 2)\n",
    "Y_reshape.shape\n",
    "\n",
    "Y = Y_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (4437, 50, 50, 3) (4437, 50, 50, 3)\n",
      "Testing Data Shape: (1110, 50, 50, 3) (1110, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Reduce Sample Size for DeBugging\n",
    "X_train = X_train[0:30000] \n",
    "Y_train =Y_train[0:30000]\n",
    "X_test = X_test[0:30000] \n",
    "Y_test = Y_test[0:30000]\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 256.0\n",
    "X_test = X_test / 256.0\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape, X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Identify IDC+ or IDC-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 875,906\n",
      "Trainable params: 875,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      " 34/138 [======>.......................] - ETA: 27s - loss: 0.6962 - acc: 0.5239"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-885049ff4265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m proghistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n\u001b[0;32m     38\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     callbacks=[TensorBoard(log_dir='mytensorboard')])\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "img_rows, img_cols = X.shape[1],X.shape[2]\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset \n",
    "                             samplewise_center=False,  # set each sample mean to 0\n",
    "                             featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                             samplewise_std_normalization=False,  # divide each input by its std\n",
    "                             zca_whitening=False,  # apply ZCA whitening\n",
    "                             rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                             width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                             height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                             horizontal_flip=True,  # randomly flip images\n",
    "                             vertical_flip=True)  # randomly flip images\n",
    "\n",
    "proghistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=epochs,validation_data = [X_test,Y_test],\n",
    "                    callbacks=[TensorBoard(log_dir='mytensorboard')])\n",
    "\n",
    "preds = model.predict(X)\n",
    "T = preds[0][0]\n",
    "F = preds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.binary,classes=None):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    xlocations = np.array(range(len(classes)))\n",
    "    plt.xticks(xlocations, classes, rotation=90)\n",
    "    plt.yticks(xlocations, classes)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test,Y_test, verbose=0)\n",
    "print('\\nKeras CNN #1C - accuracy:', score[1],'\\n')\n",
    "y_pred = model.predict(X_test)\n",
    "map_characters = {0: 'Tumor(-)', 1: 'Tumor(+)'}\n",
    "print('\\n', sklearn.metrics.classification_report(np.where(Y_test > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')    \n",
    "score = model.evaluate(X_test,Y_test, verbose=0)\n",
    "Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "Y_true = np.argmax(Y_test,axis = 1) \n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(proghistory):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(proghistory.history['loss'])\n",
    "    plt.plot(proghistory.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(proghistory.history['acc'])\n",
    "    plt.plot(proghistory.history['val_acc'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(proghistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 100\n",
    "num_classes = 2\n",
    "epochs = 500\n",
    "\n",
    "def vgg16_model(input_shape = (50,50,3)):\n",
    "    vgg16 = VGG16(include_top = False, weights = 'imagenet', input_shape = input_shape)\n",
    "    \n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    last = vgg16.output\n",
    "   \n",
    "    x = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')\n",
    "    x = MaxPooling2D(pool_size=(2, 2))\n",
    "    x = Conv2D(128, (3, 3), activation='relu')\n",
    "    x = Dropout(0.25)\n",
    "    x = Conv2D(256, (3, 3), activation='relu')\n",
    "    x = MaxPooling2D(pool_size=(2, 2))\n",
    "    x = Dropout(0.25)\n",
    "    x = Flatten()(last)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2, activation = 'softmax')(x)\n",
    "    \n",
    "    model = Model(inputs = vgg16.input, outputs = x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "vgg16 = vgg16_model()\n",
    "vgg16.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adadelta(), metrics = ['accuracy'])\n",
    "vgg16.fit_generator(datagen.flow(X_train,Y_train, batch_size=20),\n",
    "                        steps_per_epoch=len(X_train) / 30, epochs=epochs,validation_data = [X_test,Y_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_vgg16 = vgg16_model()\n",
    "model_vgg16.summary()\n",
    "model_vgg16.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 0\n",
    "r = 100\n",
    "for i in range(r):\n",
    "    p = model.predict(np.expand_dims(X_test[i],axis=0))\n",
    "    a = Y_test[i]\n",
    "    print(p,a)\n",
    "    if p[0][0]< p[0][1] and a[0]< a[1]:\n",
    "        m = m+1\n",
    "    elif p[0][0]> p[0][1] and a[0] > a[1]:\n",
    "        m = m+1\n",
    "    \n",
    "    print(m/r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X)\n",
    "T = preds[0][0]\n",
    "F = preds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Generate Reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if T>F:\n",
    "    im1 = X1\n",
    "    Img = cv2.resize(im1, (256,256), interpolation=cv2.INTER_CUBIC)\n",
    "    kernel_2 = np.ones((2, 2), np.uint8)  # 2x2的卷积核\n",
    "    kernel_3 = np.ones((3, 3), np.uint8)  # 3x3的卷积核\n",
    "    kernel_4 = np.ones((4, 4), np.uint8)  # 4x4的卷积核\n",
    "    if Img is not None:  # 判断图片是否读入\n",
    "        HSV = cv2.cvtColor(Img, cv2.COLOR_BGR2HSV)  # 把RGB图像转换为HSV格式\n",
    "        Lower = np.array([90, 25, 0])  # 要识别颜色的下限\n",
    "        Upper = np.array([154, 255, 225])  # 要识别的颜色的上限\n",
    "        # mask是把HSV图片中在颜色范围内的区域变成白色，其他区域变成黑色\n",
    "        mask = cv2.inRange(HSV, Lower, Upper)\n",
    "        # 下面四行是用卷积进行滤波\n",
    "        erosion = cv2.erode(mask, kernel_4, iterations=1)\n",
    "        erosion = cv2.erode(erosion, kernel_4, iterations=1)\n",
    "        dilation = cv2.dilate(erosion, kernel_4, iterations=1)\n",
    "        dilation = cv2.dilate(dilation, kernel_4, iterations=1)\n",
    "        # target是把原图中的非目标颜色区域去掉剩下的图像\n",
    "        target = cv2.bitwise_and(Img, Img, mask=dilation)\n",
    "        # 将滤波后的图像变成二值图像放在binary中\n",
    "        ret, binary = cv2.threshold(dilation, 127, 255, cv2.THRESH_BINARY)\n",
    "        # 在binary中发现轮廓，轮廓按照面积从小到大排列\n",
    "        contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        p = 0\n",
    "        for i in contours:  # 遍历所有的轮廓\n",
    "            x, y, w, h = cv2.boundingRect(i)  # 将轮廓分解为识别对象的左上角坐标和宽、高\n",
    "            # 在图像上画上矩形（图片、左上角坐标、右下角坐标、颜色、线条宽度）\n",
    "            cv2.rectangle(Img, (x, y), (x + w, y + h), (0, 255,), 2)\n",
    "            # 给识别对象写上标号\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(Img, str(p), (x - 10, y + 10), font, 0.5, (0, 0, 255), 1)  # 加减10是调整字符位置\n",
    "            p += 1\n",
    "        print('There are', p, 'potential tumor areas')  # 终端输出目标数量\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    # plt.subplot(n_rows, n_cols, plot_num)\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(X1)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off') \n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(Img)\n",
    "    plt.title('BC+')\n",
    "    plt.axis('off') \n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(target)\n",
    "    plt.title('Target')\n",
    "    plt.axis('off') \n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(mask)\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off') \n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(dilation)\n",
    "    plt.title('Prod')\n",
    "    plt.axis('off') \n",
    "    \n",
    "else:\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(X1)\n",
    "    plt.title('BC-')\n",
    "    plt.axis('off') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
