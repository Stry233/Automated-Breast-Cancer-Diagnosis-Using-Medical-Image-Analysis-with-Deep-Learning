{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "080fa63d-2ad7-4702-a972-61dfa453872c",
    "_uuid": "1f9069e7a456e3c1a75f148234f6433076305a76"
   },
   "source": [
    "**Predicting IDC in Breast Cancer Histology Images**\n",
    "\n",
    "Breast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer.  Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n",
    "\n",
    "The goal of this script is to identify IDC when it is present in otherwise unlabeled histopathology images.  The dataset consists of approximately five thousand 50x50 pixel RGB digital images of H&E-stained breast histopathology samples that are labeled as either IDC or non-IDC. These numpy arrays are small patches that were extracted from digital images of breast tissue samples.  The breast tissue contains many cells but only some of them are cancerous.  Patches that are labeled \"1\" contain cells that are characteristic of invasive ductal carcinoma.  For more information about the data, see https://www.ncbi.nlm.nih.gov/pubmed/27563488 and http://spie.org/Publications/Proceedings/Paper/10.1117/12.2043872.\n",
    "\n",
    "For more information about IDC and breast cancer, please review the following publications: \n",
    "* https://www.ncbi.nlm.nih.gov/pubmed/27864452\n",
    "* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3893344/\n",
    "* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4952020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9104593b-f406-449d-9feb-dc5baa146db7",
    "_uuid": "d96e080ca9ca3f1ded5f4432286f80a31a32fe5a"
   },
   "source": [
    "*Step 1: Import Modules*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8114fd92-8576-41f7-b636-698011a0ada4",
    "_kg_hide-input": true,
    "_uuid": "5fda999ecf2c3e94831ac467dd96fabf2d5a9401"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'imresize' from 'scipy.misc' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-080cdc70a5ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'imresize' from 'scipy.misc' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\misc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.misc import imresize, imread\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c281e7a5-5d0a-4c19-b5d1-a0364ce7276b",
    "_uuid": "0693c954882cb02125f00973f93772edc93ee275"
   },
   "source": [
    "*Step 2: Load Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "488707f3-c33c-42db-8344-47353b21e931",
    "_uuid": "1cca29752ae8ff62de50236aebcdc091bd446619"
   },
   "outputs": [],
   "source": [
    "X = np.load('../input/X.npy') # images\n",
    "Y = np.load('../input/Y.npy') # labels associated to images (0 = no IDC, 1 = IDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6881bc4-94d2-450c-b70a-840f6b43548b",
    "_uuid": "19734095c942f5844d42b149d6ce93e67dae620d"
   },
   "source": [
    "*Step 3: Describe Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "192003f3-4bb1-45a9-b0ac-86b136079d56",
    "_uuid": "f67d30bd2c4fea8ced7a707ba71fa1931ec286e3"
   },
   "outputs": [],
   "source": [
    "def describeData(a,b):\n",
    "    print('Total number of images: {}'.format(len(a)))\n",
    "    print('Number of IDC(-) Images: {}'.format(np.sum(b==0)))\n",
    "    print('Number of IDC(+) Images: {}'.format(np.sum(b==1)))\n",
    "    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(b)))\n",
    "    print('Image shape (Width, Height, Channels): {}'.format(a[0].shape))\n",
    "describeData(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e55339c8-7b35-40be-9662-e7c071b06cbd",
    "_uuid": "61009c6bb13abf77fbe27aba4c8499392d560824"
   },
   "source": [
    "*Step 4: Plot Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fc47fb16-21df-456e-a20c-882bd7127279",
    "_uuid": "26019e32a32b341c49a214c7b23babc474752df6"
   },
   "outputs": [],
   "source": [
    "imgs0 = X[Y==0] # (0 = no IDC, 1 = IDC)\n",
    "imgs1 = X[Y==1] \n",
    "\n",
    "def plotOne(a,b):\n",
    "    \"\"\"\n",
    "    Plot one numpy array\n",
    "    \"\"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('IDC (-)')\n",
    "    plt.imshow(a[100])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('IDC (+)')\n",
    "    plt.imshow(b[100])\n",
    "plotOne(imgs0, imgs1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b45be80f-4901-47ac-8f9f-2689d57f6a99",
    "_uuid": "1aa10060590ed9357971c2f2ca2404faa7bc1824"
   },
   "outputs": [],
   "source": [
    "def plotTwo(a,b): \n",
    "    \"\"\"\n",
    "    Plot a bunch of numpy arrays sorted by label\n",
    "    \"\"\"\n",
    "    for row in range(3):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for col in range(3):\n",
    "            plt.subplot(1,8,col+1)\n",
    "            plt.title('IDC (-)')\n",
    "            plt.imshow(a[row+col])\n",
    "            plt.axis('off')       \n",
    "            plt.subplot(1,8,col+4)\n",
    "            plt.title('IDC (+)')\n",
    "            plt.imshow(b[row+col])\n",
    "            plt.axis('off')\n",
    "plotTwo(imgs0, imgs1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a32f0573-5081-4d53-8b67-b1e19b8e4b25",
    "_uuid": "17f99516ccbfc1ff9817fdce0a8481b172e8af20"
   },
   "source": [
    "*Step 4: Preprocess Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "59bda880-1d34-424f-be00-12531ec84999",
    "_uuid": "587bb44d8c9b1028f882f600b5cfc8fbf1504aea"
   },
   "outputs": [],
   "source": [
    "def plotHistogram(a):\n",
    "    \"\"\"\n",
    "    Plot histogram of RGB Pixel Intensities\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(a)\n",
    "    plt.axis('off')\n",
    "    plt.title('IDC(+)' if Y[1] else 'IDC(-)')\n",
    "    histo = plt.subplot(1,2,2)\n",
    "    histo.set_ylabel('Count')\n",
    "    histo.set_xlabel('Pixel Intensity')\n",
    "    n_bins = 30\n",
    "    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n",
    "    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n",
    "    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\n",
    "plotHistogram(X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "191233d4-e313-49c5-9277-806dd71e131b",
    "_uuid": "37b8f4cdca6e8d3687555eb69e41ae49ebb7cde7"
   },
   "source": [
    "The data is scaled from 0 to 256 but we want it to be scaled from 0 to 1.  This will make the data compatible with a wide variety of different classification algorithms.\n",
    "\n",
    "We also want to set aside 20% of the data for k-fold cross-validation testing.  This will make the trained model less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5d3cdc63-c803-4f41-8f90-665ca03a7506",
    "_uuid": "59fcd56ba5ae49065ef3143163e7001d7d00103d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Reduce Sample Size for DeBugging\n",
    "X_train = X_train[0:30000] \n",
    "Y_train = Y_train[0:30000]\n",
    "X_test = X_test[0:30000] \n",
    "Y_test = Y_test[0:30000]\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 256.0\n",
    "X_test = X_test / 256.0\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape, X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c09f56fa-5483-4139-8ae2-a47cebe25ff8",
    "_uuid": "941926042260afe7e4be9ae287f4e938c064cc04"
   },
   "outputs": [],
   "source": [
    "plotHistogram(X_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8e0a97eb-3d6e-45a2-b675-fc2cdb1f41b0",
    "_uuid": "5cf87765a3e101e012ae347a3b27f23ebecedf36"
   },
   "source": [
    "Now the data is scaled from 0 to 1.\n",
    "\n",
    "Next we can try using some standard classification algorithms to predict whether or not IDC is present in each given sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f2edc92a-e89b-48d9-9ac8-5cd6b3699476",
    "_uuid": "9efaee02b056d85e32411114195ade342a2f8f37"
   },
   "source": [
    "*Step 5: Evaluate Classification Algorithms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "08b866bb-e9f7-4bd8-8950-b009d91cfbfa",
    "_uuid": "5756fb439fcfa7555c6d39c075d81b2aa1742b97"
   },
   "outputs": [],
   "source": [
    "# Make Data 1D for compatability with standard classifiers\n",
    "\n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e592d39a-f2ef-4fc7-b098-7d8e033f1566",
    "_uuid": "adbb109329a9298ac50779ad652d2626a4eecb68"
   },
   "outputs": [],
   "source": [
    "#runLogisticRegression\n",
    "def runLogisticRegression(a,b,c,d):\n",
    "    \"\"\"Run LogisticRegression w/ Kfold CV\"\"\"\n",
    "    model = LogisticRegression()\n",
    "    model.fit(a,b)\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    accuracy = model_selection.cross_val_score(model, c,d, cv=kfold, scoring='accuracy')\n",
    "    mean = accuracy.mean() \n",
    "    stdev = accuracy.std()\n",
    "    print('LogisticRegression - Training set accuracy: %s (%s)' % (mean, stdev))\n",
    "    print('')\n",
    "runLogisticRegression(X_trainFlat, Y_train, X_testFlat, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "687a8785-824d-4b37-a739-deff752a5dc9",
    "_uuid": "64ab48ac6f2feeb7f96b2df0e40f585bf3b94320"
   },
   "outputs": [],
   "source": [
    "# Compare Performance of Classification Algorithms\n",
    "def compareABunchOfDifferentModelsAccuracy(a,b,c,d):\n",
    "    \"\"\"\n",
    "    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n",
    "    \"\"\"    \n",
    "    print('')\n",
    "    print('Compare Multiple Classifiers:')\n",
    "    print('')\n",
    "    print('K-Fold Cross-Validation Accuracy:')\n",
    "    print('')\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('SVM', SVC()))\n",
    "    models.append(('LSVM', LinearSVC()))\n",
    "    models.append(('GNB', GaussianNB()))\n",
    "    models.append(('DTC', DecisionTreeClassifier()))\n",
    "    #models.append(('GBC', GradientBoostingClassifier()))\n",
    "    #models.append(('LDA', LinearDiscriminantAnalysis()))       \n",
    "    resultsAccuracy = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        model.fit(a, b)\n",
    "        kfold = model_selection.KFold(n_splits=10)\n",
    "        accuracy_results = model_selection.cross_val_score(model, c, d, cv=kfold, scoring='accuracy')\n",
    "        resultsAccuracy.append(accuracy_results)\n",
    "        names.append(name)\n",
    "        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n",
    "        print(accuracyMessage)  \n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison: Accuracy')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(resultsAccuracy)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_ylabel('Cross-Validation: Accuracy Score')\n",
    "    plt.show()\n",
    "    return\n",
    "compareABunchOfDifferentModelsAccuracy(X_trainFlat, Y_train, X_testFlat, Y_test)\n",
    "\n",
    "\n",
    "def defineModels():\n",
    "    \"\"\"\n",
    "    This function just defines each abbreviation used in the previous function (e.g. LR = Logistic Regression)\n",
    "    \"\"\"\n",
    "    print('')\n",
    "    print('LR = LogisticRegression')\n",
    "    print('RF = RandomForestClassifier')\n",
    "    print('KNN = KNeighborsClassifier')\n",
    "    print('SVM = Support Vector Machine SVC')\n",
    "    print('LSVM = LinearSVC')\n",
    "    print('GNB = GaussianNB')\n",
    "    print('DTC = DecisionTreeClassifier')\n",
    "    #print('GBC = GradientBoostingClassifier')\n",
    "    #print('LDA = LinearDiscriminantAnalysis')\n",
    "    print('')\n",
    "    return\n",
    "defineModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8946daa7-fa21-4922-953f-28e67596351f",
    "_uuid": "c0dce9fa7ba1b2e22370e320affe7847cbce559c"
   },
   "source": [
    "With the Support Vector Machine we are getting ~75% accuracy. Next I will plot a confusion matrix for the results that were produced by the Support Vector Machine in order to verify that we do not have too many false positives.  I will also plot a learning curve to see if our model is overfitting or if our model has high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8dd4e3e2-2523-4651-8976-70f60036693e",
    "_uuid": "447cdc390ee5518448fd181022a13bdca1f3a49f"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "def plotLotsOfLearningCurves(a,b):\n",
    "    \"\"\"Plot a bunch of learning curves http://scikit-learn.org/stable/modules/learning_curve.html\"\"\"\n",
    "    models = []\n",
    "    models.append(('Support Vector Machine', SVC()))\n",
    "    for name, model in models:\n",
    "        plot_learning_curve(model, 'Learning Curve For %s Classifier'% (name), a,b, (0.5,1), 10)\n",
    "plotLotsOfLearningCurves(X_trainFlat, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "077a7197-fbd6-4447-8492-402aaf9bf3b5",
    "_uuid": "4495b5c20e77ec309b3efaa72e721ae13668f590"
   },
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Run SVC w/ Confusion Matrix\n",
    "def runSVCconfusion(a,b,c,d):\n",
    "    \"\"\"Run SVC w/ Kfold CV + Confusion Matrix\"\"\"\n",
    "    model = SVC()\n",
    "    model.fit(a, b)\n",
    "    prediction = model.predict(c)\n",
    "    kfold = model_selection.KFold(n_splits=10)\n",
    "    accuracy = model_selection.cross_val_score(model, c,d, cv=kfold, scoring='accuracy')\n",
    "    mean = accuracy.mean() \n",
    "    stdev = accuracy.std()\n",
    "    print('\\nSupport Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev),\"\\n\")\n",
    "    cnf_matrix = confusion_matrix(d, prediction)\n",
    "    np.set_printoptions(precision=2)\n",
    "    class_names = [\"Diagnosis\" \"IDC(-)\", \"Diagnosis\" \"IDC(+)\"]\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.show()\n",
    "runSVCconfusion(X_trainFlat, Y_train, X_testFlat, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "08fbc2c0-38fe-4781-bea8-3ceb0c2cb599",
    "_uuid": "1c8c3724812719457cc7f72435e07b3e3ed5fe22"
   },
   "source": [
    "Here in these confusion plots the Y-Axis represents the True labels [\"IDC(-)\" or \"IDC(+)\"] while the X-Axis represents the Predicted labels (generated by the Support Vector Machine).  Ideally, the predicted labels will be the same as the idea labels.  This is actually pretty good!  But on the learning curve you can see that the training score tracks very closely to the cross-validation score and this makes me suspicious that the model might be overfitting.  And anyways... we should be able to improve our model's accuracy by using neural networks.  Next I will use the original 2-D data and I will try to solve this classification problem by using 2D convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d6663fb1-7a6c-444b-a052-9a190fedf4d2",
    "_uuid": "c693af7e9f5d9b993f7a0b8dc8d5a8ccc7d21fd7"
   },
   "outputs": [],
   "source": [
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 2)\n",
    "Y_test = to_categorical(Y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "876bdd91-a49b-45d6-b06d-da33ceed6388",
    "_uuid": "b4269b87124dac817d0d424f7a99f5f4bd3d18c4"
   },
   "outputs": [],
   "source": [
    "# Special callback to see learning curves\n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "\n",
    "def plotKerasLearningCurve():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "433ec78e-1dce-4b38-a0ff-d4da7d57a5a3",
    "_uuid": "157e713a4f734314cd96b2adb85c391c3afff433"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-29987121b131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mconfusion_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_characters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mrunKerasCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mplotKerasLearningCurve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "def runKerasCNN(a,b,c,d):\n",
    "    \"\"\"\n",
    "    https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 300  \n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    x_train = a\n",
    "    y_train = b\n",
    "    x_test = c\n",
    "    y_test = d   \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('\\nKeras CNN #1A - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c) \n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "runKerasCNN(X_train, Y_train,  X_test, Y_test)\n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26da5d07-8cc0-41c5-b5e9-c8e3ded1e1a7",
    "_uuid": "346cfe785b2f4b8803ce3e77184108ca52c67bd5"
   },
   "source": [
    "The confusion matrix illustrates that this model is predicting IDC(+) too often and the learning curve illustrates that the validation score is consistently less than the traning score.  Together, these results suggest that our model may have some bias.\n",
    "\n",
    "I will try using different artificial neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6b5f37c6-a1cc-4648-9239-d81e97cb3514",
    "_uuid": "8c6eb55d9654d901fe380bc6232db0f0d4ac4bcb"
   },
   "outputs": [],
   "source": [
    "def runAnotherKeras(a, b,c,d):\n",
    "    # my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out   \n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 12\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu', input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(1024, activation = \"relu\"))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = \"softmax\"))\n",
    "    optimizer = RMSprop(lr=0.001, decay=1e-6)\n",
    "    model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(a,b,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(c,d),callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #2 - accuracy:', score[1], '\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='') \n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "runAnotherKeras(X_train, Y_train, X_test, Y_test)\n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d669f339-35c9-4f5f-b308-97398762bf27",
    "_uuid": "a999917f98d8f4fdf3a51c3921789b62835dc3b7"
   },
   "source": [
    "The confusion matrix illustrates that this model is predicting IDC(-) too often and the learning curve illustrates that the validation score is consistently less than the traning score.  Together, these results suggest that our model suffers from high bias.\n",
    "\n",
    "I will try using another network architecture and I will also include a data augmentation step in our to try to decrease the bias in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eec880dd-086f-4c0b-88af-fbdaa58fd2c7",
    "_uuid": "067e0665da108abfa3dacd4353bcacd1ca22de2a"
   },
   "outputs": [],
   "source": [
    "def kerasAugmentation(a,b,c,d):\n",
    "    img_rows, img_cols = 50,50\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 12\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    datagen.fit(a)\n",
    "    model.fit_generator(datagen.flow(a,b, batch_size=32),\n",
    "                        steps_per_epoch=len(a) / 32, epochs=epochs, validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #3B - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "kerasAugmentation(X_train, Y_train, X_test, Y_test)\n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5bd1f3fd-3996-48b1-8e49-db820e99721e",
    "_uuid": "3f554740068badd620841101fde68f808cd52c24"
   },
   "source": [
    "This model picked IDC(+) every single time which suggests a high bias, but the learning curve suggests that there maybe some overfitting.  Either way, this model will not work.\n",
    "\n",
    "I will try another model now where I change the network architecture but retain the data augmentation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b7b17a1a-b7b3-4abc-a812-f2f6b60c0a88",
    "_uuid": "d0ae41a9906bfa3a441c3249516739f2c4baa908"
   },
   "outputs": [],
   "source": [
    "def runAnotherKerasAugmentedConfusion(a,b,c,d):\n",
    "    # my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out \n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 16\n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu', input_shape = input_shape))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 86, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(1024, activation = \"relu\"))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation = \"softmax\"))\n",
    "    optimizer = RMSprop(lr=0.001, decay=1e-6)\n",
    "    model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    datagen.fit(a)\n",
    "    model.fit_generator(datagen.flow(a,b, batch_size=32),steps_per_epoch=len(a) / 32, epochs=epochs, validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #2B - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')    \n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "runAnotherKerasAugmentedConfusion(X_train, Y_train, X_test, Y_test)   \n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f80503f-90fa-4e20-9eea-b15245abc9ce",
    "_uuid": "45c5124c1d6630edc2614cfbd88b1ea5160b3991"
   },
   "source": [
    "The confusion matrix illustrates that this model is predicting IDC(-) far too often and the learning curve illustrates that the validation score is consistently less than the traning score.  Together, these results suggest that our model suffers from high bias despite containing a data augmentation step.\n",
    "\n",
    "I will try using another network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2f4c1341-37d2-4a60-a9ba-98c48f4227db",
    "_uuid": "848f4e4f7466c72c32af7fbb4c33d145815497f0"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "def yetAnotherKeras(a,b,c,d):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(50, 50, 3))) # first layer : convolution\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3))) # second layer : pooling (reduce the size of the image per 3) \n",
    "    model.add(Conv2D(32, (5, 5), activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='sigmoid')) # output 1 value between 0 and 1 : probability to have cancer\n",
    "    model.summary()\n",
    "    model.compile(loss=keras.losses.binary_crossentropy, # Use binary crossentropy as a loss function  \n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(a,b,\n",
    "              batch_size=128,\n",
    "              epochs=12,\n",
    "              verbose=1,\n",
    "              validation_data = [c,d],\n",
    "            callbacks = [MetricsCheckpoint('logs')])\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    y_pred = model.predict(c)\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "yetAnotherKeras(X_train,Y_train,X_test,Y_test)\n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "318e5e72-6608-4256-be7d-0a4974ba6320",
    "_uuid": "3986ecf8f6c17056d455b0af1d1847a996ef7695"
   },
   "source": [
    "This is a decent result.  The learning curve here suggests that our model does not have too much bias.  If anything, the model may be overfitting a bit, given the close relationship between the training and validation scores.\n",
    "\n",
    "I will try using a different network architecture and once again I will also include a data augmentation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "37f23b9b-4089-412b-89a4-56339f68931c",
    "_uuid": "6c318102474676d0d33a1a89e877ee02cfd790df"
   },
   "outputs": [],
   "source": [
    "def runKerasCNNAugment(a,b,c,d):\n",
    "    \"\"\"\n",
    "    Run Keras CNN: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 12\n",
    "    img_rows, img_cols = X_train.shape[1],X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    x_train = a\n",
    "    y_train = b\n",
    "    x_test = c\n",
    "    y_test = d\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    model.fit_generator(datagen.flow(a,b, batch_size=32), # 把图片用datagen进行处理\n",
    "                        steps_per_epoch=len(a) / 32, epochs=epochs, validation_data = [c, d])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #1C - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    Y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "    Y_true = np.argmax(Y_test,axis = 1) \n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(map_characters.values())) \n",
    "runKerasCNNAugment(X_train, Y_train,  X_test, Y_test)\n",
    "plotKerasLearningCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9bf36145-0148-4be6-8ac8-f1d42d48c776",
    "_uuid": "d14cbbce7cde54c5381c5c2ec0a96aad3af66297"
   },
   "source": [
    "This is our best result yet.  76% accuracy and a distribution of predicted labels that is similar to the distribtion of actual labels (50/50).   The learning curve suggests that there is not too much overfitting given the different shapes of the training and cross-validation curves, and both the confusion matrix and the learning curve suggest that the model does not have high bias.  But with only two categories (IDC negative/IDC plus), we should hope to do better than 80% accuracy.  Soon I will experiment with different data augmentation approaches in an attempt to improve our model's accuracy. In the future, tools like this can be used to save time, cut costs, and increase the accuracy of imaging-based diagnostic approaches in the healthcare industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce2339ef-12f5-4fa7-8a0f-27a766502e33",
    "_uuid": "2d6ac5ecb8fb75f40f1ba67a957fe536f2af8f2b",
    "collapsed": true
   },
   "source": [
    "To Do:\n",
    "1) Improve data visualization\n",
    "2) Optimize data augmentation\n",
    "3) Optimize NN architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
